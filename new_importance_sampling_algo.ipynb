{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "\n",
    "from utils import *\n",
    "\n",
    "MAINPATH = \"/Users/research/projects/gmp22/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the posterior chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHO_OBS = (1.401, 0.1401)\n",
    "\n",
    "test = 'test6-s20-b0.48-e0.05-w90'\n",
    "date = '09Feb22'\n",
    "\n",
    "trace_S = pd.read_csv(MAINPATH + 'test_data/'+test+'_S.csv')\n",
    "trace_EW = pd.read_csv(MAINPATH + 'test_data/'+test+'_EW.csv')\n",
    "\n",
    "traces = {'S':trace_S, 'EW':trace_EW}\n",
    "rho_samples_mcmc = trace_S[\"rho\"].values\n",
    "\n",
    "ess_mcmc = az.ess(rho_samples_mcmc)\n",
    "ess_mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define importance sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imp_sample(rho_array, rho_obs, norm=True):\n",
    "    '''\n",
    "    Perform standard importance sampling from rho_tilde --> {e, omega}\n",
    "    \n",
    "    Args:\n",
    "    rho_array [array]: sampled data for pseudo-density rho_tilde\n",
    "    rho_obs [tuple]: values of the true stellar density and its uncertainty\n",
    "    norm [bool] : True to normalize weights before output (default=True)\n",
    "    \n",
    "    Output:\n",
    "    weights [array]: importance sampling weights\n",
    "    ecc [array]: random values drawn uniformly from 0 to 1, with array length = len(rho_array)\n",
    "    omega [array]:random values drawn uniformly from -pi/2 to 3pi/2, with array length = len(rho_array)\n",
    "    '''\n",
    "    \n",
    "    ecc = np.random.uniform(0., 1, len(rho_array))\n",
    "    omega = np.random.uniform(-0.5*np.pi, 1.5*np.pi, len(rho_array))\n",
    "\n",
    "    g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "    rho = rho_array / g ** 3\n",
    "\n",
    "    log_weight = -0.5 * ((rho - rho_obs[0]) / rho_obs[1]) ** 2\n",
    "    weights = np.exp(log_weight - np.max(log_weight))\n",
    "\n",
    "    if norm:\n",
    "        weights /= np.sum(weights)\n",
    "    \n",
    "    return weights, ecc, omega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial comparison of 'ecc' from log(T) model vs. e-omega-rho model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, ecc, omega = imp_sample(rho_samples_mcmc, RHO_OBS)\n",
    "\n",
    "plt.figure();\n",
    "plt.hist(ecc, weights=weights, bins=100, density=True, alpha=0.4, label='log(T) model');\n",
    "plt.hist(trace_EW['ecc'], bins=100, density=True, alpha=0.4, label='e-omega-rho model', color='C2');\n",
    "plt.ylabel('count density', fontsize=16)\n",
    "plt.xlabel('ecc', fontsize=16)\n",
    "plt.legend(fontsize=14);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's our more sophisticated ESS-based sampling scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate KDE bandwidth -- only needs to be done once\n",
    "bw = get_bw(rho_samples_mcmc)\n",
    "print(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples = {}\n",
    "samples[\"rho\"] = np.array([])\n",
    "samples[\"ecc\"] = np.array([])\n",
    "samples[\"omega\"] = np.array([])\n",
    "samples[\"weights\"] = np.array([])\n",
    "\n",
    "n_up = np.min([int(5e4),10*len(rho_samples_mcmc)])\n",
    "f_eff = ess_mcmc/len(rho_samples_mcmc)\n",
    "\n",
    "ess_imp = 0\n",
    "d_ess_lim = 0.1\n",
    "\n",
    "run_num = 0\n",
    "run_lim = 10\n",
    "\n",
    "\n",
    "while ess_imp < len(rho_samples_mcmc):\n",
    "    run_num += 1\n",
    "    if run_num > run_lim:\n",
    "        print('Run limit ({0} runs) exceeded!'.format(run_lim))\n",
    "        break\n",
    "    else:\n",
    "        print('RUN {0}'.format(run_num))\n",
    "        print('drawing {0} synthetic samples'.format(n_up))\n",
    "\n",
    "    # generate synthetic samples from actual MCMC samples with PDF oversampling\n",
    "    rho = generate_synthetic_samples(rho_samples_mcmc.reshape(-1,1), [bw], n_up)[:,0]\n",
    "    \n",
    "    # use synthetic samples for importance sampling\n",
    "    weights, ecc, omega = imp_sample(rho, RHO_OBS, norm=False)\n",
    "    \n",
    "    # combine new samples with samples from previous iterations\n",
    "    rho = np.hstack([rho, samples[\"rho\"]])\n",
    "    ecc = np.hstack([ecc, samples[\"ecc\"]])\n",
    "    omega = np.hstack([omega, samples[\"omega\"]])\n",
    "    weights = np.hstack([weights, samples[\"weights\"]])\n",
    "    \n",
    "    w_norm = weights/np.sum(weights)\n",
    "    \n",
    "    # this is not the true ESS - it's the ESS if all synthetic data were uncorrelated\n",
    "    # i.e. this only account for the importance weight contribution to reduced ESS\n",
    "    ess_imp = 1 / np.sum(w_norm**2)\n",
    "\n",
    "    # use leave-out-out to measure 'd_ess' for each sample as a measure of sample importance\n",
    "    idx = np.arange(0, len(w_norm), dtype=\"int\")\n",
    "    ess_LOO = np.zeros(len(w_norm))\n",
    "    \n",
    "    for i, w in enumerate(w_norm):\n",
    "        w_ = w_norm[idx != i]\n",
    "        w_ /= np.sum(w_)\n",
    "        ess_LOO[i] = 1/np.sum(w_**2)\n",
    "        \n",
    "    d_ess = ess_imp - ess_LOO\n",
    "    \n",
    "    # keep only samples with 'd_ess' above the limit 'd_ess_lim'\n",
    "    keep = d_ess > d_ess_lim\n",
    "    \n",
    "    samples[\"rho\"] = rho[keep]\n",
    "    samples[\"ecc\"] = ecc[keep]\n",
    "    samples[\"omega\"] = omega[keep]\n",
    "    samples[\"weights\"] = weights[keep]\n",
    "    \n",
    "    print(\"keeping {0} samples (cumulative)\\n\".format(np.sum(keep)))\n",
    "    \n",
    "    # estimate how many more upsamples should be drawn\n",
    "    if run_num == 1:\n",
    "        w_ = samples[\"weights\"]/np.sum(samples[\"weights\"])\n",
    "        ess_ = 1/np.sum(w_**2)\n",
    "        \n",
    "        efficiency = ess_/n_up\n",
    "        estimated_upsamples_needed = int(n_up/efficiency)\n",
    "        cumulative_upsample_count = 0\n",
    "        \n",
    "        print(\"efficiency = {0:.3f}\".format(efficiency))\n",
    "        print(\"estimated upsamples needed: {0}\\n\".format(estimated_upsamples_needed))\n",
    "              \n",
    "    cumulative_upsample_count += n_up\n",
    "    \n",
    "    # next iteration draw betweeen 1k-50k new synthetic upsamples\n",
    "    n_up = np.max([1000, np.min([int(5e4), estimated_upsamples_needed-cumulative_upsample_count])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new 'resampled' data vs. 'original' data from log(T) method results\n",
    "plt.figure();\n",
    "plt.hist(samples[\"rho\"], bins=100, density=True, alpha=0.4, label='resampled');\n",
    "plt.hist(rho_samples_mcmc.reshape(-1), bins=100, density=True, alpha=0.2, label='original rho_tilde');\n",
    "plt.ylabel('count density', fontsize=16)\n",
    "plt.xlabel('rho_tilde', fontsize=16)\n",
    "plt.legend(fontsize=14);\n",
    "plt.show();\n",
    "\n",
    "\n",
    "# plot eccentricity distribution of new data with importance sampling vs. \n",
    "# empirical eccentricity distribution from e-omega-rho sampling\n",
    "plt.figure();\n",
    "plt.hist(samples[\"ecc\"], weights=samples[\"weights\"], bins=100, density=True, alpha=0.4, label='resampled log(T) model');\n",
    "plt.hist(trace_EW['ecc'], bins=100, density=True, alpha=0.4, label='e-omega-rho model', color='C2');\n",
    "plt.ylabel('count density', fontsize=16)\n",
    "plt.xlabel('ecc', fontsize=16)\n",
    "plt.legend(fontsize=14);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Q and $\\Delta e$ plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecut = 0.95\n",
    "\n",
    "ecc_ew = np.asarray(trace_EW[\"ecc\"][trace_EW[\"ecc\"] < ecut])\n",
    "ecc_T = samples[\"ecc\"][samples[\"ecc\"] < ecut]\n",
    "w_T = samples[\"weights\"][samples[\"ecc\"] < ecut]\n",
    "w_T /= np.sum(w_T)\n",
    "\n",
    "percs = np.arange(100.)\n",
    "perc_ew = np.zeros_like(percs)\n",
    "perc_T = np.zeros_like(percs)\n",
    "\n",
    "for i, p in enumerate(percs):\n",
    "    perc_ew[i] =  np.percentile(ecc_ew, p)\n",
    "    perc_T[i] = weighted_percentile(ecc_T, p, w_T)\n",
    "    \n",
    "perc_ew = boxcar_smooth(perc_ew, 5)\n",
    "perc_T = boxcar_smooth(perc_T, 5)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(percs, perc_T-perc_ew, \"k\")\n",
    "plt.xlabel(\"percentile\", fontsize=24)\n",
    "plt.ylabel(r\"$e_T - e_{e\\omega\\rho}$\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ew = np.array([stats.percentileofscore(ecc_ew, p, 'rank') for p in perc_ew])\n",
    "q_T = np.array([stats.percentileofscore(ecc_ew, p, 'rank') for p in perc_T])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(q_ew, q_T, \"ko\")\n",
    "plt.plot(np.linspace(0,100), np.linspace(0,100), \"r\", lw=3)\n",
    "plt.xlabel(r\"$e_{e\\omega\\rho}$ percentile\", fontsize=24)\n",
    "plt.ylabel(r\"$e_T$ percentile\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap KS/AD test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First check $\\{e, \\omega\\}$ against itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nboot = 1000\n",
    "KS_boot = np.zeros(Nboot)\n",
    "AD_boot = np.zeros(Nboot)\n",
    "\n",
    "for i in range(Nboot):\n",
    "    if i % 100 == 0: print(i)\n",
    "        \n",
    "    ecc_boot = np.random.choice(ecc_ew, size=len(ecc_ew), replace=True)\n",
    "    \n",
    "    KS_boot[i] = stats.ks_2samp(ecc_ew, ecc_boot)[1]\n",
    "    AD_boot[i] = anderson_2samp(ecc_ew, ecc_boot)[1]\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(np.log10(KS_boot), bins=np.linspace(-1.5,0,16), alpha=0.4, label=\"KS\")\n",
    "plt.hist(np.log10(AD_boot), bins=np.linspace(-1.5,0,16), alpha=0.4, label=\"AD\")\n",
    "plt.xlabel(r\"$\\log_{10}p$\", fontsize=20)\n",
    "plt.ylabel(\"counts\", fontsize=20)\n",
    "plt.legend(fontsize=16)\n",
    "plt.title(r\"Comparing $e_{e\\omega\\rho}$ against itself\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check $\\{e, \\omega\\}$ against $\\log T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nboot = 1000\n",
    "KS_boot = np.zeros(Nboot)\n",
    "AD_boot = np.zeros(Nboot)\n",
    "\n",
    "for i in range(Nboot):\n",
    "    if i % 100 == 0: print(i)\n",
    "        \n",
    "    ecc_boot = np.random.choice(ecc_T, size=len(ecc_T), p=w_T, replace=True)\n",
    "    \n",
    "    KS_boot[i] = stats.ks_2samp(ecc_ew, ecc_boot)[1]\n",
    "    AD_boot[i] = anderson_2samp(ecc_ew, ecc_boot)[1]\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(np.log10(KS_boot), bins=np.linspace(-8,0,17), alpha=0.4, label=\"KS\")\n",
    "plt.hist(np.log10(AD_boot), bins=np.linspace(-8,0,17), alpha=0.4, label=\"AD\")\n",
    "plt.xlabel(r\"$\\log_{10}p$\", fontsize=20)\n",
    "plt.ylabel(\"counts\", fontsize=20)\n",
    "plt.legend(fontsize=16)\n",
    "plt.title(r\"$e_{e\\omega\\rho}$ against $e_{T}$\", fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
